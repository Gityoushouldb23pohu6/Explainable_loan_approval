# -*- coding: utf-8 -*-
"""xgboost_genai

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mh97YTtOYyBA0i3ikQUs_Ych1AaoqcP4
"""

import torch
import pandas as pd
import numpy as np
import streamlit as st
import xgboost as xgb

from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import accuracy_score
from transformers import AutoTokenizer, AutoModelForCausalLM


TARGET_COL = "loan_status"
MODEL_NAME = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
MAX_NEW_TOKENS = 200
RANDOM_STATE = 42
N_SPLITS = 5


@st.cache_resource
def load_genai_model():
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        device_map="auto"
    )
    model.eval()
    return tokenizer, model


tokenizer, llm = load_genai_model()


st.set_page_config(page_title="Explainable Loan Approval System", layout="wide")
st.title("Explainable Loan Approval System")
st.write("XGBoost-based prediction with cross-validation and language-model explanations")

uploaded_file = st.file_uploader("Upload loan dataset", type="csv")

if not uploaded_file:
    st.stop()


data = pd.read_csv(uploaded_file)

st.subheader("Dataset preview")
st.dataframe(data.head())

labels = data[TARGET_COL]
features = data.drop(columns=[TARGET_COL])

for col in features.columns:
    if features[col].dtype == "object":
        features[col] = features[col].fillna(features[col].mode()[0])
    else:
        features[col] = features[col].fillna(features[col].median())

features = pd.get_dummies(features, drop_first=True)

xgb_model = xgb.XGBClassifier(
    n_estimators=200,
    max_depth=4,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    eval_metric="logloss",
    random_state=RANDOM_STATE
)

cv = StratifiedKFold(
    n_splits=N_SPLITS,
    shuffle=True,
    random_state=RANDOM_STATE
)

with st.spinner("Running cross-validation"):
    cv_scores = cross_val_score(
        xgb_model,
        features,
        labels,
        cv=cv,
        scoring="accuracy"
    )

st.write(
    f"Cross-validation accuracy: "
    f"{cv_scores.mean():.3f} Â± {cv_scores.std():.3f}"
)

X_train, X_test, y_train, y_test = train_test_split(
    features,
    labels,
    test_size=0.2,
    stratify=labels,
    random_state=RANDOM_STATE
)

with st.spinner("Training final model"):
    xgb_model.fit(X_train, y_train)

test_accuracy = accuracy_score(y_test, xgb_model.predict(X_test))
st.write(f"Hold-out test accuracy: {test_accuracy:.3f}")

st.subheader("New loan application")

applicant = {}
for col in features.columns:
    applicant[col] = st.number_input(col, value=0.0)

if st.button("Evaluate"):
    applicant_df = pd.DataFrame([applicant])
    applicant_df = applicant_df.reindex(columns=features.columns, fill_value=0)

    probabilities = xgb_model.predict_proba(applicant_df)[0]
    approved = probabilities[1] >= 0.5

    decision = "Approved" if approved else "Rejected"
    confidence = probabilities[1] if approved else probabilities[0]

    st.write(f"Decision: {decision}")
    st.write(f"Confidence: {confidence:.2f}")

    importances = xgb_model.feature_importances_
    top_idx = np.argsort(importances)[-5:][::-1]

    signals = []
    for idx in top_idx:
        name = features.columns[idx]
        value = applicant_df.iloc[0, idx]
        signals.append(f"{name} = {value}")

    prompt = f"""
The following loan decision was produced by a machine learning model.

Decision: {decision}
Confidence: {confidence:.2f}

Key contributing factors:
{chr(10).join(signals)}

Explain this decision clearly and concisely without adding new assumptions.
"""

    inputs = tokenizer(prompt, return_tensors="pt").to(llm.device)

    with torch.no_grad():
        output = llm.generate(
            **inputs,
            max_new_tokens=MAX_NEW_TOKENS,
            temperature=0.2,
            do_sample=False
        )

    explanation = tokenizer.decode(output[0], skip_special_tokens=True)

    st.subheader("Explanation")
    st.write(explanation)